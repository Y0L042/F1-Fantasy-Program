{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6402645-7e16-406b-b798-01cb6d63d8c8",
   "metadata": {},
   "source": [
    "# F1 Fantasy Program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf7bd9-28c2-440e-8eff-14c9e15df9e1",
   "metadata": {},
   "source": [
    "## ReadMe\n",
    "\n",
    "The goal of this program is to webscrape, and gather, F1 data. The data would then be saved, transformed, and analysed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c428dd45",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc057482",
   "metadata": {},
   "source": [
    "#### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca433e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('modules')\n",
    "\n",
    "from datetime import datetime\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01b9e7b7",
   "metadata": {},
   "source": [
    "#### FastF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd0949b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastf1_extract as ff1\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "686645fc",
   "metadata": {},
   "source": [
    "#### Webscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1c44542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import f1_webscrape_extract as f1ws"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "237855cd",
   "metadata": {},
   "source": [
    "#### MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e3bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import f1stats_database as f1db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cf2d64-c108-46f4-907b-aee072da55db",
   "metadata": {},
   "source": [
    "### Ignore Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71df1cbd-a74d-4efb-b3a2-05483bbb2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c06f4ad3",
   "metadata": {},
   "source": [
    "# Global Variables & Settings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af66db0d",
   "metadata": {},
   "source": [
    "### Loaded races logfile name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a9bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_races_logfile = 'loaded_races_logfile.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2de62f9a",
   "metadata": {},
   "source": [
    "# Save to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39997bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(target_csv, data_to_load):\n",
    "    data_to_load.to_csv(target_csv) #, mode='a', header=False, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d898ddd",
   "metadata": {},
   "source": [
    "# Extract: FastF1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfa9a30d",
   "metadata": {},
   "source": [
    "### Log Loaded Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15a1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loaded_race(race_number, loaded_races_logfile = 'loaded_races_logfile.txt'):\n",
    "\ttimestamp_format = '%Y-%h-%d-%H:%M:%S' #Year-Monthname-Day-Hour-Minute-Second\n",
    "\tnow = datetime.now()\n",
    "\ttimestamp = now.strftime(timestamp_format)\n",
    "\twith open(loaded_races_logfile, 'a') as log:\n",
    "\t\tlog.write(timestamp + ','+str(race_number)+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abbfe434",
   "metadata": {},
   "source": [
    "### Read logfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca8b4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_loaded_races_logfile(loaded_races_logfile = 'loaded_races_logfile.txt'):\n",
    "\tloaded_races = []\n",
    "\twith open(loaded_races_logfile, 'r') as log:\n",
    "\t\tfor line in log.readlines():\n",
    "\t\t\t# print(line)\n",
    "\t\t\tfields = line.strip().split(',')\n",
    "\t\t\tloaded_races.append(int(fields[1]))\n",
    "\t\n",
    "\treturn loaded_races"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f892ec0",
   "metadata": {},
   "source": [
    "### Find missing races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28cae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_races(races_list: list, season: int):\n",
    "\t# given a list of races, and the count of completed races, it finds the numbers that are missing\n",
    "\tif len(races_list) == 0:\n",
    "\t\traces_list = [0]\n",
    "\tmissing = set(range(min(races_list), ff1.get_completed_events_count(season)+1)) - set(races_list)\n",
    "\treturn list(missing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4836f3a",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b2d039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1ws.TODO_SORT_INTO_FUNCTIONS()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2a459bf9-b94e-4e4c-8d55-04e1f220b5dd",
   "metadata": {},
   "source": [
    "# Transform: Extract select columns from Data, into CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8db6afe3-c642-4446-8e30-d1de3b1c5a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ff1_data_into_csv(season=2023, session_type='R'):\n",
    "\tevent_number = 0\n",
    "\n",
    "\tloaded_races = read_loaded_races_logfile()\n",
    "\t# print(loaded_races)\n",
    "\tmissing_races = find_missing_races(loaded_races, season)\n",
    "\t# print(missing_races)\n",
    "\n",
    "\traces_csv_folder = 'races_csv/'\n",
    "\tfor event_num in missing_races:\n",
    "\t\tsession = ff1.get_session_data(season, event_num, session_type) # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! TODO only relevant columns must be saved into the csv files\n",
    "\t\tsave_to_csv(races_csv_folder+str(session.event.gp)+'_'+str(session.event.date)+'.csv',session.results)\n",
    "\t\tlog_loaded_race(event_num) # maybe move this to after the data is actually loaded into the database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bce7641",
   "metadata": {},
   "source": [
    "# MySQL Database f1stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c1c210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MySQL database and return db_object\n",
    "mydb = f1db.connect_to_database()\n",
    "\n",
    "\n",
    "\n",
    "# Execute Functions\n",
    "def execute_db_insert_functions():\n",
    "    with mydb.cursor() as mycursor:\n",
    "        try:\n",
    "            # Call insert functions here\n",
    "            mydb.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            mydb.rollback()\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "def get_standings():\n",
    "    # Define the API endpoint URL\n",
    "    url = \"https://fantasy.formula1.com/api/f1/2021/standings\"\n",
    "\n",
    "    try:\n",
    "        # Make a GET request to the API endpoint\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the JSON data from the response\n",
    "            data = response.json()\n",
    "\n",
    "            # Access the data as needed\n",
    "            standings = data['standings']\n",
    "            return standings\n",
    "        else:\n",
    "            # Handle the error if the request was not successful\n",
    "            print(f\"Failed to retrieve data from the API. Response code: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        # Handle any unexpected exceptions\n",
    "        print(f\"An error occurred while connecting to the API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_from_dataframe(cursor, dataframe, table_name):\n",
    "    for index, row in dataframe.iterrows():\n",
    "        data_row = row.to_dict()\n",
    "        if table_name == 'teams':\n",
    "            f1db.insert_into_teams(cursor, data_row)\n",
    "        elif table_name == 'team_price_history':\n",
    "            f1db.insert_into_team_price_history(cursor, data_row, team_id_fk=row['team_id_fk'])\n",
    "        elif table_name == 'drivers':\n",
    "            f1db.insert_into_drivers(cursor, data_row, team_id_fk=row['team_id_fk'])\n",
    "        elif table_name == 'driver_price_history':\n",
    "            f1db.insert_into_driver_price_history(cursor, data_row, driver_id_fk=row['driver_id_fk'])\n",
    "        elif table_name == 'circuit':\n",
    "            f1db.insert_into_circuit(cursor, data_row)\n",
    "        elif table_name == 'race':\n",
    "            f1db.insert_into_race(cursor, data_row, circuit_id_fk=row['circuit_id_fk'])\n",
    "        elif table_name == 'race_results':\n",
    "            f1db.insert_into_race_results(cursor, data_row, race_id_fk=row['race_id_fk'], driver_id_fk=row['driver_id_fk'], team_id_fk=row['team_id_fk'])\n",
    "        elif table_name == 'weather':\n",
    "            f1db.insert_into_weather(cursor, data_row, race_id_fk=row['race_id_fk'])\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid table name: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74000fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "def generate_uuid():\n",
    "    return uuid.uuid4().bytes # this will return a random binary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17433cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc8\\xd6\\x0fR5\\x14@4\\x89\\xb5~L\\xe2[F\\x03'\n"
     ]
    }
   ],
   "source": [
    "print(generate_uuid())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d358b097",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33fb0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message):\n",
    "    timestamp_format = '%Y-%h-%d-%H:%M:%S' #Year-Monthname-Day-Hour-Minute-Second\n",
    "    now = datetime.now()\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open('ETL_log.txt', 'a') as log: \n",
    "        log.write(timestamp + ','+str(message)+'\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a857ec9f",
   "metadata": {},
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57448e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "def etl():\n",
    "    ff1.enable_cache()\n",
    "\n",
    "    # Extract Data\n",
    "    # log('Start Extract Step')\n",
    "    extract_ff1_data_into_csv() # double check this later\n",
    "    race_csv_files = glob.glob('/races_csv/*.csv')\n",
    "    list_of_race_dfs = []\n",
    "\n",
    "    # Load the list of already processed files\n",
    "    if os.path.exists('processed_files.txt'):\n",
    "        with open('processed_files.txt', 'r') as f:\n",
    "            processed_files = f.read().splitlines()\n",
    "    else:\n",
    "        processed_files = []\n",
    "\n",
    "    for file in race_csv_files:\n",
    "        if file not in processed_files:\n",
    "            df = pd.read_csv(file)\n",
    "            list_of_race_dfs.append(df) # add race to list of race dfs\n",
    "            # Add the file to the list of processed files\n",
    "            processed_files.append(file)\n",
    "\n",
    "    # Save the updated list of processed files\n",
    "    with open('processed_files.txt', 'w') as f:\n",
    "        f.write('\\n'.join(processed_files))\n",
    "\n",
    "\n",
    "    # insert webscraping\n",
    "    # log('End Extract Step')\n",
    "\n",
    "\n",
    "\n",
    "    # Transform\n",
    "    # log('Start Transfrom Step')\n",
    "\n",
    "    # Create dataframes for each table\n",
    "    \n",
    "    teams_df = [['team_name']]\n",
    "    # teams_df = \n",
    "    # team_price_history_df = \n",
    "    # drivers_df = \n",
    "    # driver_price_history_df = \n",
    "    # circuit_df = \n",
    "    # race_df = \n",
    "    # race_results_df = \n",
    "    # weather_df = \n",
    "\n",
    "    # log('End Transform Step')\n",
    "\n",
    "\n",
    "    # Load\n",
    "    # log('Start Load Step')\n",
    "    # Connect to the database\n",
    "    mydb = f1db.connect_to_database()\n",
    "    cursor = mydb.cursor()\n",
    "\n",
    "\n",
    "    # Insert data into the tables\n",
    "    insert_data_from_dataframe(cursor, teams_df, 'teams')\n",
    "    insert_data_from_dataframe(cursor, team_price_history_df, 'team_price_history')\n",
    "    insert_data_from_dataframe(cursor, drivers_df, 'drivers')\n",
    "    insert_data_from_dataframe(cursor, driver_price_history_df, 'driver_price_history')\n",
    "    insert_data_from_dataframe(cursor, circuit_df, 'circuit')\n",
    "    insert_data_from_dataframe(cursor, race_df, 'race')\n",
    "    insert_data_from_dataframe(cursor, race_results_df, 'race_results')\n",
    "    insert_data_from_dataframe(cursor, weather_df, 'weather')\n",
    "\n",
    "    # Commit changes and close the connection\n",
    "    mydb.commit()\n",
    "    cursor.close()\n",
    "    mydb.close()\n",
    "    \n",
    "    # log('End Load Step')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
